{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3351f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "from numpy import array\n",
    "from numpy import transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "49208163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor(1., requires_grad=True)\n",
      "y: tensor(1., grad_fn=<PowBackward0>)\n",
      "x.grad: tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# input is scalar, output is scalar\n",
    "x = tensor(1.0, requires_grad=True)\n",
    "print('x:', x)\n",
    "y = x**2\n",
    "print('y:', y)\n",
    "y.backward() # this is the same as y.backward(tensor(1.0))\n",
    "print('x.grad:', x.grad)\n",
    "\n",
    "# want: dy/dx\n",
    "# dy/dx = d(x^2)/dx = 2x where x is 1, so\n",
    "# gradient is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d73537ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J: [[2.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[2*x]])\n",
    "print('J:', J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2e4d9c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: [[1]]\n",
      "v*J: [[2.]]\n"
     ]
    }
   ],
   "source": [
    "v = array([[1,]])\n",
    "print('v:', v)\n",
    "print('v*J:', v@J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "14163c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor(1., requires_grad=True)\n",
      "y: tensor(1., grad_fn=<PowBackward0>)\n",
      "None\n",
      "x.grad: tensor(200.)\n"
     ]
    }
   ],
   "source": [
    "# input is scalar, output is scalar, non-default gradient\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "print('x:', x)\n",
    "y = x**2\n",
    "print('y:', y)\n",
    "gradient_value=100.0\n",
    "print(y.backward(torch.tensor(gradient_value)))\n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cd8dd3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J: [[2.]]\n",
      "v: [[100.]]\n",
      "v*J: [[200.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[2*x]])\n",
    "print('J:', J)\n",
    "\n",
    "v = array([[gradient_value,]])\n",
    "print('v:', v)\n",
    "print('v*J:', v@J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5bdab0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor(3., grad_fn=<AddBackward0>)\n",
      "x.grad: tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# input is vector, output is scalar\n",
    "x = tensor([1.,2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = sum(x)\n",
    "print('y:', y)\n",
    "y.backward()\n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e477c18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      " [[1 1]]\n"
     ]
    }
   ],
   "source": [
    "J = array([[1,1]])\n",
    "print('J:\\n', J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2d33eb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: [[1]]\n",
      "v*J: [[1 1]]\n"
     ]
    }
   ],
   "source": [
    "v = array([[1]])\n",
    "print('v:', v)\n",
    "print('v*J:', v@J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "36f49c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor([ 3., 12.], grad_fn=<MulBackward0>)\n",
      "x.grad tensor([ 6., 12.])\n"
     ]
    }
   ],
   "source": [
    "# input is vector, output is vector\n",
    "x = tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = 3*x**2\n",
    "print('y:', y)\n",
    "gradient_value = [1., 1.]\n",
    "y.backward(tensor(gradient_value))\n",
    "print('x.grad', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "43e1f045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      " [[ 6.  0.]\n",
      " [ 0. 12.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[6*x[0], 0],\n",
    "           [0,6*x[1]]])\n",
    "print('J:\\n', J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9c9cb56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v trans: [[1.]\n",
      " [1.]]\n",
      "J trans: [[ 6.  0.]\n",
      " [ 0. 12.]]\n",
      "v: [[1. 1.]]\n",
      "v*J: [[ 6. 12.]]\n"
     ]
    }
   ],
   "source": [
    "v = array([gradient_value])\n",
    "# v = 1 10 100\n",
    "\n",
    "# J = 6 0\n",
    "#      0 12\n",
    "print('v trans:', transpose(v)) \n",
    "print('J trans:', transpose(J))\n",
    "\n",
    "print('v:', v)\n",
    "print('v*J:', v@J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ea23f562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor([ 3., 12.], grad_fn=<MulBackward0>)\n",
      "x.grad: tensor([  6., 120.])\n"
     ]
    }
   ],
   "source": [
    "# input is vector, output is vector, non-one gradient\n",
    "x = tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = 3*x**2\n",
    "print('y:', y)\n",
    "gradient_value = [1., 10.]\n",
    "y.backward(tensor(gradient_value))\n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c911445f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      " [[ 6.  0.]\n",
      " [ 0. 12.]]\n",
      "v: [[ 1. 10.]]\n",
      "v*J: [[  6. 120.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[6*x[0], 0], [0, 6*x[1]]])\n",
    "print('J:\\n', J)\n",
    "\n",
    "v = array([gradient_value])\n",
    "print('v:', v)\n",
    "print('v*J:', v@J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8ba0b076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor([ 3., 17., 20.], grad_fn=<CopySlices>)\n",
      "x.grad: tensor([  26., 1240.])\n"
     ]
    }
   ],
   "source": [
    "# input is vector, output is vector - another example\n",
    "x = tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = torch.empty(3)\n",
    "y[0]=3*x[0]**2\n",
    "y[1]=x[0]**2 + 2*x[1]**3\n",
    "y[2]=10*x[1]\n",
    "print('y:', y)\n",
    "gradient_value = [1., 10., 100.,]\n",
    "y.backward(tensor(gradient_value))\n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "95c50ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      " [[ 6.  0.]\n",
      " [ 2. 24.]\n",
      " [ 0. 10.]]\n",
      "v: [1. 1. 1.]\n",
      "v*J: [ 8. 34.]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[6*x[0], 0],\n",
    "          [2*x[0], 6*x[1]**2],\n",
    "          [0, 10]])\n",
    "print('J:\\n', J)\n",
    "\n",
    "# the jacobian matrix\n",
    "# 6 0         \n",
    "# 2 24    \n",
    "# 0 10         \n",
    "\n",
    "v = array([1., 1., 1.])\n",
    "# vector\n",
    "# 1\n",
    "# 10\n",
    "# 100\n",
    "\n",
    "print('v:', v)\n",
    "print('v*J:', v@J)\n",
    "# output ??\n",
    "# 8\n",
    "# 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "938e06dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([2., 3.], requires_grad=True)\n",
      "b: tensor([6., 4.], requires_grad=True)\n",
      "Q: tensor([-12.,  65.], grad_fn=<SubBackward0>)\n",
      "a.grad: tensor([36., 81.])\n",
      "b.grad: tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "a = tensor([2., 3.], requires_grad=True)\n",
    "b = tensor([6., 4.], requires_grad=True)\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    "\n",
    "Q = 3*a**3 - b**2\n",
    "# Jacobian matrix\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "\n",
    "print('Q:', Q)\n",
    "gradient = tensor([1., 1.])\n",
    "Q.backward(gradient=gradient)\n",
    "print('a.grad:', a.grad)\n",
    "print('b.grad:', b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d5d4cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d76874cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[1., 2., 3., 4.]], requires_grad=True)\n",
      "z: tensor([[2., 4., 6., 8.]], grad_fn=<MulBackward0>)\n",
      "loss: tensor([20.], grad_fn=<SumBackward1>)\n",
      "tensor([[2., 0., 0., 0.]])\n",
      "tensor([[0., 2., 0., 0.]])\n",
      "tensor([[2., 2., 2., 2.]])\n",
      "tensor([[2., 2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.FloatTensor([[1, 2, 3, 4]]), requires_grad=True)\n",
    "z = 2*x\n",
    "loss = z.sum(dim=1)\n",
    "print('x:', x)\n",
    "print('z:', z)\n",
    "print('loss:', loss)\n",
    "\n",
    "# do backward for first element of z\n",
    "z.backward(torch.FloatTensor([[1,0,0,0]]), retain_graph=True)\n",
    "print(x.grad.data)\n",
    "x.grad.data.zero_() #remove gradient in x.grad, or it will accumulate\n",
    "\n",
    "# do backward for second element of z\n",
    "z.backward(torch.FloatTensor([[0,1,0,0]]), retain_graph=True)\n",
    "print(x.grad.data)\n",
    "x.grad.data.zero_()\n",
    "\n",
    "# do backward for all elements of z, with weight equal to the derivitive of\n",
    "# loss with respect to z_1, z_2, z_3, and z_4\n",
    "z.backward(torch.FloatTensor([[1,1,1,1]]), retain_graph=True)\n",
    "print(x.grad.data)\n",
    "x.grad.data.zero_()\n",
    "\n",
    "# or we can directly backprop using loss\n",
    "loss.backward()\n",
    "print(x.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7f95ba8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.], requires_grad=True)\n",
      "tensor([2., 2., 2.], grad_fn=<MulBackward0>)\n",
      "tensor([10., 10., 10.], grad_fn=<MulBackward0>)\n",
      "tensor([10., 10., 10.], grad_fn=<MulBackward0>)\n",
      "tensor(30., grad_fn=<SumBackward0>)\n",
      "tensor([20., 20., 20.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, requires_grad=True)\n",
    "print(x)\n",
    "# 1 1 1\n",
    "y = 2*x**2 # y = 2x^2\n",
    "print(y)\n",
    "# 2 2 2\n",
    "z = 5*y # z = 5y\n",
    "print(z)\n",
    "# 10 10 10\n",
    " # L = z + z + z\n",
    "# 30\n",
    "print(z)\n",
    "# z.backward(tensor([1,1,1]))\n",
    "loss = z.sum()\n",
    "print(loss)\n",
    "loss.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# want dz/dx\n",
    "# dz/dy = d(5y)/dy = 5\n",
    "# dy/dx = d(2x^2)/dx = 4x\n",
    "# dz/dx = dz/dy * dy/dx = 5 * 4x\n",
    "# where x is = 1 so we have\n",
    "# 5 * 4(1) = 20\n",
    "# thus the grad is 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ffc932",
   "metadata": {},
   "source": [
    "Implementing basic NN \n",
    "\n",
    "We are essentially trying to calculate the correct weights so our output is essentially, λx.2x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7f06d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7fc528d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 2: w = 1.680, loss = 4.79999924\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 4: w = 1.949, loss = 0.12288000\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 6: w = 1.992, loss = 0.00314570\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 8: w = 1.999, loss = 0.00008053\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 10: w = 2.000, loss = 0.00000206\n",
      "prediction after taining f(5) = 9.999\n"
     ]
    }
   ],
   "source": [
    "# f = w * x\n",
    "# f = 2 * x\n",
    "\n",
    "X = array([1,2,3,4], dtype=np.float32)\n",
    "Y = array([2,4,6,8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "# loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "    \n",
    "# gradient\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N 2x (w*x - y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return ((2*x)@(y_predicted-y)).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "    \n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f'prediction after taining f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55634fc3",
   "metadata": {},
   "source": [
    "That stuff put with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca2cdaf",
   "metadata": {},
   "source": [
    "1. Design our model (input size, output size, forward pass)\n",
    "2. Construct loss and optimizer\n",
    "3. Training loop\n",
    "    - forward pass: compute prediction\n",
    "    - backward pass: gradients\n",
    "    - update our weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cf7720cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "52742f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5) = -1.638\n",
      "epoch 1: w = 0.030, loss = 40.81784821\n",
      "epoch 11: w = 1.499, loss = 1.12504435\n",
      "epoch 21: w = 1.740, loss = 0.09408082\n",
      "epoch 31: w = 1.785, loss = 0.06362558\n",
      "epoch 41: w = 1.797, loss = 0.05927593\n",
      "epoch 51: w = 1.804, loss = 0.05580907\n",
      "epoch 61: w = 1.810, loss = 0.05256020\n",
      "epoch 71: w = 1.815, loss = 0.04950097\n",
      "epoch 81: w = 1.821, loss = 0.04661974\n",
      "epoch 91: w = 1.826, loss = 0.04390620\n",
      "prediction after taining f(5) = 9.651\n"
     ]
    }
   ],
   "source": [
    "X = tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y = tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "# forward\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "# model = nn.Linear(input_size, output_size,)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "# model prediction\n",
    "X_test = tensor([5.])\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# SGD = Stochastic Gradient Descent \n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # gradients = backward pass\n",
    "    l.backward() #dL/dw\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        [w,b] = model.parameters()\n",
    "        print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f'prediction after taining f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6bc665",
   "metadata": {},
   "source": [
    "Implementing linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f77b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "#0) prepare data\n",
    "#1) model\n",
    "#2) loss and optimizer\n",
    "#3) training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35c966f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss: 4465.1558\n",
      "epoch: 20, loss: 3331.4573\n",
      "epoch: 30, loss: 2510.6118\n",
      "epoch: 40, loss: 1915.6809\n",
      "epoch: 50, loss: 1484.0814\n",
      "epoch: 60, loss: 1170.7000\n",
      "epoch: 70, loss: 942.9738\n",
      "epoch: 80, loss: 777.3692\n",
      "epoch: 90, loss: 656.8582\n",
      "epoch: 100, loss: 569.1072\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preparing data\n",
    "X_numpy, Y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
    "\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "Y = torch.from_numpy(Y_numpy.astype(np.float32))\n",
    "Y = Y.view(Y.shape[0], 1)   \n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# model\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# loss and optimizer\n",
    "learning_rate = .01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "# training loops\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # forward pass and loss\n",
    "    yp = model(X)\n",
    "    loss = criterion(yp, Y)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    optimizer.step()\n",
    "    \n",
    "    # empty gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss: {loss.item():.4f}')\n",
    "    \n",
    "# plot\n",
    "predicted = model(X).detach().numpy()\n",
    "plt.plot(X_numpy, Y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de070368",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "1. Design model (input, output size, forward pass)\n",
    "2. Construct loss and optimizer\n",
    "3. training loop\n",
    "    - forward pass: prediction and loss\n",
    "    - backward pass: gradients\n",
    "    - update weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9701e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f496a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 30\n",
      "epoch: 10, loss = 0.2618\n",
      "epoch: 20, loss = 0.2176\n",
      "epoch: 30, loss = 0.1850\n",
      "epoch: 40, loss = 0.1614\n",
      "epoch: 50, loss = 0.1439\n",
      "epoch: 60, loss = 0.1305\n",
      "epoch: 70, loss = 0.1200\n",
      "epoch: 80, loss = 0.1115\n",
      "epoch: 90, loss = 0.1045\n",
      "epoch: 100, loss = 0.0986\n",
      "accuracy = 0.8947\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1234)\n",
    "\n",
    "# scale\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "# model\n",
    "# f = wx + b, sigmoid at the end\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "    def forward(self, x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n",
    "\n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "# loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterian = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# training\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    optimizer.step()\n",
    "    \n",
    "    # empty gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "# plot\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ccc14cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "data = datasets.load_breast_cancer()\n",
    "print(data.target_names)\n",
    "print(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abba05d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
