{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3351f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "from numpy import array\n",
    "from numpy import transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49208163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor(1., requires_grad=True)\n",
      "y: tensor(1., grad_fn=<PowBackward0>)\n",
      "x.grad: tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# input is scalar, output is scalar\n",
    "x = tensor(1.0, requires_grad=True)\n",
    "print('x:', x)\n",
    "y = x**2\n",
    "print('y:', y)\n",
    "y.backward() # this is the same as y.backward(tensor(1.0))\n",
    "print('x.grad:', x.grad)\n",
    "\n",
    "# want: dy/dx\n",
    "# dy/dx = d(x^2)/dx = 2x where x is 1, so\n",
    "# gradient is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73537ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J: [[2.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[2*x]])\n",
    "print('J:', J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e4d9c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: [[1]]\n",
      "v*J: [[2.]]\n"
     ]
    }
   ],
   "source": [
    "v = array([[1,]])\n",
    "print('v:', v)\n",
    "print('v*J:', v@J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14163c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor(1., requires_grad=True)\n",
      "y: tensor(1., grad_fn=<PowBackward0>)\n",
      "None\n",
      "x.grad: tensor(200.)\n"
     ]
    }
   ],
   "source": [
    "# input is scalar, output is scalar, non-default gradient\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "print('x:', x)\n",
    "y = x**2\n",
    "print('y:', y)\n",
    "gradient_value=100.0\n",
    "print(y.backward(torch.tensor(gradient_value)))\n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd8dd3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J: [[2.]]\n",
      "v: [[100.]]\n",
      "v*J: [[200.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[2*x]])\n",
    "print('J:', J)\n",
    "\n",
    "v = array([[gradient_value,]])\n",
    "print('v:', v)\n",
    "print('v*J:', v@J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bdab0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor(3., grad_fn=<AddBackward0>)\n",
      "x.grad: tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# input is vector, output is scalar\n",
    "x = tensor([1.,2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = sum(x)\n",
    "print('y:', y)\n",
    "y.backward()\n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e477c18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      " [[1 1]]\n"
     ]
    }
   ],
   "source": [
    "J = array([[1,1]])\n",
    "print('J:\\n', J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d33eb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: [[1]]\n",
      "v*J: [[1 1]]\n"
     ]
    }
   ],
   "source": [
    "v = array([[1]])\n",
    "print('v:', v)\n",
    "print('v*J:', v@J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36f49c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor([ 3., 12.], grad_fn=<MulBackward0>)\n",
      "x.grad tensor([ 6., 12.])\n"
     ]
    }
   ],
   "source": [
    "# input is vector, output is vector\n",
    "x = tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = 3*x**2\n",
    "print('y:', y)\n",
    "gradient_value = [1., 1.]\n",
    "y.backward(tensor(gradient_value))\n",
    "print('x.grad', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e1f045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      " [[ 6.  0.]\n",
      " [ 0. 12.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[6*x[0], 0],\n",
    "           [0,6*x[1]]])\n",
    "print('J:\\n', J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c9cb56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v trans: [[1.]\n",
      " [1.]]\n",
      "J trans: [[ 6.  0.]\n",
      " [ 0. 12.]]\n",
      "v: [[1. 1.]]\n",
      "v*J: [[ 6. 12.]]\n"
     ]
    }
   ],
   "source": [
    "v = array([gradient_value])\n",
    "# v = 1 10 100\n",
    "\n",
    "# J = 6 0\n",
    "#      0 12\n",
    "print('v trans:', transpose(v)) \n",
    "print('J trans:', transpose(J))\n",
    "\n",
    "print('v:', v)\n",
    "print('v*J:', v@J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea23f562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor([ 3., 12.], grad_fn=<MulBackward0>)\n",
      "x.grad: tensor([  6., 120.])\n"
     ]
    }
   ],
   "source": [
    "# input is vector, output is vector, non-one gradient\n",
    "x = tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = 3*x**2\n",
    "print('y:', y)\n",
    "gradient_value = [1., 10.]\n",
    "y.backward(tensor(gradient_value))\n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c911445f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      " [[ 6.  0.]\n",
      " [ 0. 12.]]\n",
      "v: [[ 1. 10.]]\n",
      "v*J: [[  6. 120.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[6*x[0], 0], [0, 6*x[1]]])\n",
    "print('J:\\n', J)\n",
    "\n",
    "v = array([gradient_value])\n",
    "print('v:', v)\n",
    "print('v*J:', v@J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ba0b076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor([ 3., 17., 20.], grad_fn=<CopySlices>)\n",
      "x.grad: tensor([  26., 1240.])\n"
     ]
    }
   ],
   "source": [
    "# input is vector, output is vector - another example\n",
    "x = tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = torch.empty(3)\n",
    "y[0]=3*x[0]**2\n",
    "y[1]=x[0]**2 + 2*x[1]**3\n",
    "y[2]=10*x[1]\n",
    "print('y:', y)\n",
    "gradient_value = [1., 10., 100.,]\n",
    "y.backward(tensor(gradient_value))\n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95c50ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      " [[ 6.  0.]\n",
      " [ 2. 24.]\n",
      " [ 0. 10.]]\n",
      "v: [1. 1. 1.]\n",
      "v*J: [ 8. 34.]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[6*x[0], 0],\n",
    "          [2*x[0], 6*x[1]**2],\n",
    "          [0, 10]])\n",
    "print('J:\\n', J)\n",
    "\n",
    "# the jacobian matrix\n",
    "# 6 0         \n",
    "# 2 24    \n",
    "# 0 10         \n",
    "\n",
    "v = array([1., 1., 1.])\n",
    "# vector\n",
    "# 1\n",
    "# 10\n",
    "# 100\n",
    "\n",
    "print('v:', v)\n",
    "print('v*J:', v@J)\n",
    "# output ??\n",
    "# 8\n",
    "# 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "938e06dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([2., 3.], requires_grad=True)\n",
      "b: tensor([6., 4.], requires_grad=True)\n",
      "Q: tensor([-12.,  65.], grad_fn=<SubBackward0>)\n",
      "a.grad: tensor([36., 81.])\n",
      "b.grad: tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "a = tensor([2., 3.], requires_grad=True)\n",
    "b = tensor([6., 4.], requires_grad=True)\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    "\n",
    "Q = 3*a**3 - b**2\n",
    "# Jacobian matrix\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "\n",
    "print('Q:', Q)\n",
    "gradient = tensor([1., 1.])\n",
    "Q.backward(gradient=gradient)\n",
    "print('a.grad:', a.grad)\n",
    "print('b.grad:', b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5d4cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d76874cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[1., 2., 3., 4.]], requires_grad=True)\n",
      "z: tensor([[2., 4., 6., 8.]], grad_fn=<MulBackward0>)\n",
      "loss: tensor([20.], grad_fn=<SumBackward1>)\n",
      "tensor([[2., 0., 0., 0.]])\n",
      "tensor([[0., 2., 0., 0.]])\n",
      "tensor([[2., 2., 2., 2.]])\n",
      "tensor([[2., 2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.FloatTensor([[1, 2, 3, 4]]), requires_grad=True)\n",
    "z = 2*x\n",
    "loss = z.sum(dim=1)\n",
    "print('x:', x)\n",
    "print('z:', z)\n",
    "print('loss:', loss)\n",
    "\n",
    "# do backward for first element of z\n",
    "z.backward(torch.FloatTensor([[1,0,0,0]]), retain_graph=True)\n",
    "print(x.grad.data)\n",
    "x.grad.data.zero_() #remove gradient in x.grad, or it will accumulate\n",
    "\n",
    "# do backward for second element of z\n",
    "z.backward(torch.FloatTensor([[0,1,0,0]]), retain_graph=True)\n",
    "print(x.grad.data)\n",
    "x.grad.data.zero_()\n",
    "\n",
    "# do backward for all elements of z, with weight equal to the derivitive of\n",
    "# loss with respect to z_1, z_2, z_3, and z_4\n",
    "z.backward(torch.FloatTensor([[1,1,1,1]]), retain_graph=True)\n",
    "print(x.grad.data)\n",
    "x.grad.data.zero_()\n",
    "\n",
    "# or we can directly backprop using loss\n",
    "loss.backward()\n",
    "print(x.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f95ba8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.], requires_grad=True)\n",
      "tensor([2., 2., 2.], grad_fn=<MulBackward0>)\n",
      "tensor([10., 10., 10.], grad_fn=<MulBackward0>)\n",
      "tensor([10., 10., 10.], grad_fn=<MulBackward0>)\n",
      "tensor(30., grad_fn=<SumBackward0>)\n",
      "tensor([20., 20., 20.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, requires_grad=True)\n",
    "print(x)\n",
    "# 1 1 1\n",
    "y = 2*x**2 # y = 2x^2\n",
    "print(y)\n",
    "# 2 2 2\n",
    "z = 5*y # z = 5y\n",
    "print(z)\n",
    "# 10 10 10\n",
    " # L = z + z + z\n",
    "# 30\n",
    "print(z)\n",
    "# z.backward(tensor([1,1,1]))\n",
    "loss = z.sum()\n",
    "print(loss)\n",
    "loss.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# want dz/dx\n",
    "# dz/dy = d(5y)/dy = 5\n",
    "# dy/dx = d(2x^2)/dx = 4x\n",
    "# dz/dx = dz/dy * dy/dx = 5 * 4x\n",
    "# where x is = 1 so we have\n",
    "# 5 * 4(1) = 20\n",
    "# thus the grad is 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ffc932",
   "metadata": {},
   "source": [
    "Implementing basic NN \n",
    "\n",
    "We are essentially trying to calculate the correct weights so our output is essentially, λx.2x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f06d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fc528d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 2: w = 1.680, loss = 4.79999924\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 4: w = 1.949, loss = 0.12288000\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 6: w = 1.992, loss = 0.00314570\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 8: w = 1.999, loss = 0.00008053\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 10: w = 2.000, loss = 0.00000206\n",
      "prediction after taining f(5) = 9.999\n"
     ]
    }
   ],
   "source": [
    "# f = w * x\n",
    "# f = 2 * x\n",
    "\n",
    "X = array([1,2,3,4], dtype=np.float32)\n",
    "Y = array([2,4,6,8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "# loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "    \n",
    "# gradient\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N 2x (w*x - y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return ((2*x)@(y_predicted-y)).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "    \n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f'prediction after taining f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55634fc3",
   "metadata": {},
   "source": [
    "That stuff put with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca2cdaf",
   "metadata": {},
   "source": [
    "1. Design our model (input size, output size, forward pass)\n",
    "2. Construct loss and optimizer\n",
    "3. Training loop\n",
    "    - forward pass: compute prediction\n",
    "    - backward pass: gradients\n",
    "    - update our weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf7720cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52742f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5) = -4.029\n",
      "epoch 1: w = -0.598, loss = 53.88356781\n",
      "epoch 11: w = 1.092, loss = 1.88112199\n",
      "epoch 21: w = 1.379, loss = 0.50734675\n",
      "epoch 31: w = 1.439, loss = 0.44510663\n",
      "epoch 41: w = 1.462, loss = 0.41835266\n",
      "epoch 51: w = 1.479, loss = 0.39398056\n",
      "epoch 61: w = 1.495, loss = 0.37104812\n",
      "epoch 71: w = 1.509, loss = 0.34945127\n",
      "epoch 81: w = 1.524, loss = 0.32911131\n",
      "epoch 91: w = 1.538, loss = 0.30995524\n",
      "prediction after taining f(5) = 9.074\n"
     ]
    }
   ],
   "source": [
    "X = tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y = tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "# forward\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "# model = nn.Linear(input_size, output_size,)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "# model prediction\n",
    "X_test = tensor([5.])\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# SGD = Stochastic Gradient Descent \n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # gradients = backward pass\n",
    "    l.backward() #dL/dw\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        [w,b] = model.parameters()\n",
    "        print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f'prediction after taining f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6bc665",
   "metadata": {},
   "source": [
    "Implementing linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f77b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "#0) prepare data\n",
    "#1) model\n",
    "#2) loss and optimizer\n",
    "#3) training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35c966f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss: 4487.4214\n",
      "epoch: 20, loss: 3344.4834\n",
      "epoch: 30, loss: 2517.9756\n",
      "epoch: 40, loss: 1919.6293\n",
      "epoch: 50, loss: 1486.0146\n",
      "epoch: 60, loss: 1171.4800\n",
      "epoch: 70, loss: 943.1234\n",
      "epoch: 80, loss: 777.1990\n",
      "epoch: 90, loss: 656.5477\n",
      "epoch: 100, loss: 568.7564\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preparing data\n",
    "X_numpy, Y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
    "\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "Y = torch.from_numpy(Y_numpy.astype(np.float32))\n",
    "Y = Y.view(Y.shape[0], 1)   \n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# model\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# loss and optimizer\n",
    "learning_rate = .01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "# training loops\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # forward pass and loss\n",
    "    yp = model(X)\n",
    "    loss = criterion(yp, Y)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    optimizer.step()\n",
    "    \n",
    "    # empty gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss: {loss.item():.4f}')\n",
    "    \n",
    "# plot\n",
    "predicted = model(X).detach().numpy()\n",
    "plt.plot(X_numpy, Y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de070368",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "1. Design model (input, output size, forward pass)\n",
    "2. Construct loss and optimizer\n",
    "3. training loop\n",
    "    - forward pass: prediction and loss\n",
    "    - backward pass: gradients\n",
    "    - update weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9701e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f496a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 30\n",
      "epoch: 10, loss = 0.2072\n",
      "epoch: 20, loss = 0.1787\n",
      "epoch: 30, loss = 0.1574\n",
      "epoch: 40, loss = 0.1411\n",
      "epoch: 50, loss = 0.1284\n",
      "epoch: 60, loss = 0.1182\n",
      "epoch: 70, loss = 0.1098\n",
      "epoch: 80, loss = 0.1029\n",
      "epoch: 90, loss = 0.0970\n",
      "epoch: 100, loss = 0.0919\n",
      "accuracy = 0.8947\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1234)\n",
    "\n",
    "# scale\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "# model\n",
    "# f = wx + b, sigmoid at the end\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "    def forward(self, x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n",
    "\n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "# loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterian = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# training\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    optimizer.step()\n",
    "    \n",
    "    # empty gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "# plot\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ccc14cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "data = datasets.load_breast_cancer()\n",
    "# print(data.target_names)\n",
    "# print(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abba05d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
