{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3351f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "from numpy import array\n",
    "from numpy import transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49208163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor(1., requires_grad=True)\n",
      "y: tensor(1., grad_fn=<PowBackward0>)\n",
      "x.grad: tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# input is scalar, output is scalar\n",
    "x = tensor(1.0, requires_grad=True)\n",
    "print('x:', x)\n",
    "y = x**2\n",
    "print('y:', y)\n",
    "y.backward() # this is the same as y.backward(tensor(1.0))\n",
    "print('x.grad:', x.grad)\n",
    "\n",
    "# want: dy/dx\n",
    "# dy/dx = d(x^2)/dx = 2x where x is 1, so\n",
    "# gradient is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73537ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J: [[2.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[2*x]])\n",
    "print('J:', J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e4d9c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: [[1]]\n",
      "v*J: [[2.]]\n"
     ]
    }
   ],
   "source": [
    "v = array([[1,]])\n",
    "print('v:', v)\n",
    "print('v*J:', v@J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14163c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor(1., requires_grad=True)\n",
      "y: tensor(1., grad_fn=<PowBackward0>)\n",
      "None\n",
      "x.grad: tensor(200.)\n"
     ]
    }
   ],
   "source": [
    "# input is scalar, output is scalar, non-default gradient\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "print('x:', x)\n",
    "y = x**2\n",
    "print('y:', y)\n",
    "gradient_value=100.0\n",
    "print(y.backward(torch.tensor(gradient_value)))\n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd8dd3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J: [[2.]]\n",
      "v: [[100.]]\n",
      "v*J: [[200.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[2*x]])\n",
    "print('J:', J)\n",
    "\n",
    "v = array([[gradient_value,]])\n",
    "print('v:', v)\n",
    "print('v*J:', v@J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bdab0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor(3., grad_fn=<AddBackward0>)\n",
      "x.grad: tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# input is vector, output is scalar\n",
    "x = tensor([1.,2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = sum(x)\n",
    "print('y:', y)\n",
    "y.backward()\n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e477c18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      " [[1 1]]\n"
     ]
    }
   ],
   "source": [
    "J = array([[1,1]])\n",
    "print('J:\\n', J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d33eb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: [[1]]\n",
      "v*J: [[1 1]]\n"
     ]
    }
   ],
   "source": [
    "v = array([[1]])\n",
    "print('v:', v)\n",
    "print('v*J:', v@J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36f49c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor([ 3., 12.], grad_fn=<MulBackward0>)\n",
      "x.grad tensor([ 6., 12.])\n"
     ]
    }
   ],
   "source": [
    "# input is vector, output is vector\n",
    "x = tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = 3*x**2\n",
    "print('y:', y)\n",
    "gradient_value = [1., 1.]\n",
    "y.backward(tensor(gradient_value))\n",
    "print('x.grad', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e1f045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      " [[ 6.  0.]\n",
      " [ 0. 12.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[6*x[0], 0],\n",
    "           [0,6*x[1]]])\n",
    "print('J:\\n', J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c9cb56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v trans: [[1.]\n",
      " [1.]]\n",
      "J trans: [[ 6.  0.]\n",
      " [ 0. 12.]]\n",
      "v: [[1. 1.]]\n",
      "v*J: [[ 6. 12.]]\n"
     ]
    }
   ],
   "source": [
    "v = array([gradient_value])\n",
    "# v = 1 10 100\n",
    "\n",
    "# J = 6 0\n",
    "#      0 12\n",
    "print('v trans:', transpose(v)) \n",
    "print('J trans:', transpose(J))\n",
    "\n",
    "print('v:', v)\n",
    "print('v*J:', v@J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea23f562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor([ 3., 12.], grad_fn=<MulBackward0>)\n",
      "x.grad: tensor([  6., 120.])\n"
     ]
    }
   ],
   "source": [
    "# input is vector, output is vector, non-one gradient\n",
    "x = tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = 3*x**2\n",
    "print('y:', y)\n",
    "gradient_value = [1., 10.]\n",
    "y.backward(tensor(gradient_value))\n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c911445f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      " [[ 6.  0.]\n",
      " [ 0. 12.]]\n",
      "v: [[ 1. 10.]]\n",
      "v*J: [[  6. 120.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[6*x[0], 0], [0, 6*x[1]]])\n",
    "print('J:\\n', J)\n",
    "\n",
    "v = array([gradient_value])\n",
    "print('v:', v)\n",
    "print('v*J:', v@J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ba0b076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor([ 3., 17., 20.], grad_fn=<CopySlices>)\n",
      "x.grad: tensor([  26., 1240.])\n"
     ]
    }
   ],
   "source": [
    "# input is vector, output is vector - another example\n",
    "x = tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = torch.empty(3)\n",
    "y[0]=3*x[0]**2\n",
    "y[1]=x[0]**2 + 2*x[1]**3\n",
    "y[2]=10*x[1]\n",
    "print('y:', y)\n",
    "gradient_value = [1., 10., 100.,]\n",
    "y.backward(tensor(gradient_value))\n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95c50ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      " [[ 6.  0.]\n",
      " [ 2. 24.]\n",
      " [ 0. 10.]]\n",
      "v: [1. 1. 1.]\n",
      "v*J: [ 8. 34.]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[6*x[0], 0],\n",
    "          [2*x[0], 6*x[1]**2],\n",
    "          [0, 10]])\n",
    "print('J:\\n', J)\n",
    "\n",
    "# the jacobian matrix\n",
    "# 6 0         \n",
    "# 2 24    \n",
    "# 0 10         \n",
    "\n",
    "v = array([1., 1., 1.])\n",
    "# vector\n",
    "# 1\n",
    "# 10\n",
    "# 100\n",
    "\n",
    "print('v:', v)\n",
    "print('v*J:', v@J)\n",
    "# output ??\n",
    "# 8\n",
    "# 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "938e06dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([2., 3.], requires_grad=True)\n",
      "b: tensor([6., 4.], requires_grad=True)\n",
      "Q: tensor([-12.,  65.], grad_fn=<SubBackward0>)\n",
      "a.grad: tensor([36., 81.])\n",
      "b.grad: tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "a = tensor([2., 3.], requires_grad=True)\n",
    "b = tensor([6., 4.], requires_grad=True)\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    "\n",
    "Q = 3*a**3 - b**2\n",
    "# Jacobian matrix\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "\n",
    "print('Q:', Q)\n",
    "gradient = tensor([1., 1.])\n",
    "Q.backward(gradient=gradient)\n",
    "print('a.grad:', a.grad)\n",
    "print('b.grad:', b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5d4cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d76874cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[1., 2., 3., 4.]], requires_grad=True)\n",
      "z: tensor([[2., 4., 6., 8.]], grad_fn=<MulBackward0>)\n",
      "loss: tensor([20.], grad_fn=<SumBackward1>)\n",
      "tensor([[2., 0., 0., 0.]])\n",
      "tensor([[0., 2., 0., 0.]])\n",
      "tensor([[2., 2., 2., 2.]])\n",
      "tensor([[2., 2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.FloatTensor([[1, 2, 3, 4]]), requires_grad=True)\n",
    "z = 2*x\n",
    "loss = z.sum(dim=1)\n",
    "print('x:', x)\n",
    "print('z:', z)\n",
    "print('loss:', loss)\n",
    "\n",
    "# do backward for first element of z\n",
    "z.backward(torch.FloatTensor([[1,0,0,0]]), retain_graph=True)\n",
    "print(x.grad.data)\n",
    "x.grad.data.zero_() #remove gradient in x.grad, or it will accumulate\n",
    "\n",
    "# do backward for second element of z\n",
    "z.backward(torch.FloatTensor([[0,1,0,0]]), retain_graph=True)\n",
    "print(x.grad.data)\n",
    "x.grad.data.zero_()\n",
    "\n",
    "# do backward for all elements of z, with weight equal to the derivitive of\n",
    "# loss with respect to z_1, z_2, z_3, and z_4\n",
    "z.backward(torch.FloatTensor([[1,1,1,1]]), retain_graph=True)\n",
    "print(x.grad.data)\n",
    "x.grad.data.zero_()\n",
    "\n",
    "# or we can directly backprop using loss\n",
    "loss.backward()\n",
    "print(x.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f95ba8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.], requires_grad=True)\n",
      "tensor([2., 2., 2.], grad_fn=<MulBackward0>)\n",
      "tensor([10., 10., 10.], grad_fn=<MulBackward0>)\n",
      "tensor([10., 10., 10.], grad_fn=<MulBackward0>)\n",
      "tensor(30., grad_fn=<SumBackward0>)\n",
      "tensor([20., 20., 20.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, requires_grad=True)\n",
    "print(x)\n",
    "# 1 1 1\n",
    "y = 2*x**2 # y = 2x^2\n",
    "print(y)\n",
    "# 2 2 2\n",
    "z = 5*y # z = 5y\n",
    "print(z)\n",
    "# 10 10 10\n",
    " # L = z + z + z\n",
    "# 30\n",
    "print(z)\n",
    "# z.backward(tensor([1,1,1]))\n",
    "loss = z.sum()\n",
    "print(loss)\n",
    "loss.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# want dz/dx\n",
    "# dz/dy = d(5y)/dy = 5\n",
    "# dy/dx = d(2x^2)/dx = 4x\n",
    "# dz/dx = dz/dy * dy/dx = 5 * 4x\n",
    "# where x is = 1 so we have\n",
    "# 5 * 4(1) = 20\n",
    "# thus the grad is 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ffc932",
   "metadata": {},
   "source": [
    "Implementing basic NN \n",
    "\n",
    "We are essentially trying to calculate the correct weights so our output is essentially, Î»x.2x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f06d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fc528d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 2: w = 1.680, loss = 4.79999924\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 4: w = 1.949, loss = 0.12288000\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 6: w = 1.992, loss = 0.00314570\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 8: w = 1.999, loss = 0.00008053\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 10: w = 2.000, loss = 0.00000206\n",
      "prediction after taining f(5) = 9.999\n"
     ]
    }
   ],
   "source": [
    "# f = w * x\n",
    "# f = 2 * x\n",
    "\n",
    "X = array([1,2,3,4], dtype=np.float32)\n",
    "Y = array([2,4,6,8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "# loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "    \n",
    "# gradient\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N 2x (w*x - y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return ((2*x)@(y_predicted-y)).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "    \n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f'prediction after taining f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55634fc3",
   "metadata": {},
   "source": [
    "That stuff put with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca2cdaf",
   "metadata": {},
   "source": [
    "1. Design our model (input size, output size, forward pass)\n",
    "2. Construct loss and optimizer\n",
    "3. Training loop\n",
    "    - forward pass: compute prediction\n",
    "    - backward pass: gradients\n",
    "    - update our weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf7720cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52742f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5) = -1.742\n",
      "epoch 1: w = 0.136, loss = 44.28298569\n",
      "epoch 11: w = 1.661, loss = 1.14879036\n",
      "epoch 21: w = 1.908, loss = 0.03262538\n",
      "epoch 31: w = 1.948, loss = 0.00357860\n",
      "epoch 41: w = 1.956, loss = 0.00266792\n",
      "epoch 51: w = 1.958, loss = 0.00249446\n",
      "epoch 61: w = 1.960, loss = 0.00234879\n",
      "epoch 71: w = 1.961, loss = 0.00221206\n",
      "epoch 81: w = 1.962, loss = 0.00208332\n",
      "epoch 91: w = 1.963, loss = 0.00196206\n",
      "prediction after taining f(5) = 9.926\n"
     ]
    }
   ],
   "source": [
    "X = tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y = tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "# forward\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "# model = nn.Linear(input_size, output_size,)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "# model prediction\n",
    "X_test = tensor([5.])\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# SGD = Stochastic Gradient Descent \n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # gradients = backward pass\n",
    "    l.backward() #dL/dw\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        [w,b] = model.parameters()\n",
    "        print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f'prediction after taining f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6bc665",
   "metadata": {},
   "source": [
    "Implementing linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f77b4b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "#0) prepare data\n",
    "#1) model\n",
    "#2) loss and optimizer\n",
    "#3) training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c966f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data\n",
    "X_numpy, Y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
    "\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "Y = torch.from_numpy(Y_numpy.astype(np.float32))\n",
    "Y = Y.view(Y.shape[0], 1)   \n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# model\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# loss and optimizer\n",
    "learning_rate = .01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "# training loops\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # forward pass and loss\n",
    "    yp = model(X)\n",
    "    loss = criterion(yp, Y)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    optimizer.step()\n",
    "    \n",
    "    # empty gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss: {loss.item():.4f}')\n",
    "    \n",
    "# plot\n",
    "predicted = model(X).detach().numpy()\n",
    "plt.plot(X_numpy, Y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de070368",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "1. Design model (input, output size, forward pass)\n",
    "2. Construct loss and optimizer\n",
    "3. training loop\n",
    "    - forward pass: prediction and loss\n",
    "    - backward pass: gradients\n",
    "    - update weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f496a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1234)\n",
    "\n",
    "# scale\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "# model\n",
    "# f = wx + b, sigmoid at the end\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "    def forward(self, x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n",
    "\n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "# loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterian = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# training\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    optimizer.step()\n",
    "    \n",
    "    # empty gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "# plot\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc14cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_breast_cancer()\n",
    "print(data.target_names)\n",
    "print(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abba05d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
